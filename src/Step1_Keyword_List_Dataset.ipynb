{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "R0RE-f04YwUr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdwSDhLY7Tsy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "bgRa7Yg7fg1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "k4bb_AomW_Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "8aihnWpsYe21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
      ],
      "metadata": {
        "id": "eekV7-hi7rfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "EQ8IejQyZT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset type 'datsaet'\n",
        "print(type(dataset))"
      ],
      "metadata": {
        "id": "xv_hsORIXaly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to pandas dataframe\n",
        "df_cnn_train = pd.DataFrame(dataset['train'])\n",
        "df_cnn_valid = pd.DataFrame(dataset['validation'])\n",
        "df_cnn_test = pd.DataFrame(dataset['test'])"
      ],
      "metadata": {
        "id": "W7vSaLGaXeyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate to make full dataframe\n",
        "## 데이터셋 전체에 작업해서 보내드리면 되는 것 같아서, 일단 전부 concat해뒀습니다.\n",
        "df_cnn = pd.concat([df_cnn_train, df_cnn_valid, df_cnn_test], axis=0)\n",
        "df_cnn = df_cnn.reset_index(drop=True)\n",
        "df_cnn.head()"
      ],
      "metadata": {
        "id": "_Nh8tjC1ZdvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn.shape"
      ],
      "metadata": {
        "id": "SSpmSEz_g7Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "HHM2Sy_RYiKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Model & GPU device Setting"
      ],
      "metadata": {
        "id": "EayC0AOsepoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "x9g_XJI6hIG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bloomberg/KeyBART\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"bloomberg/KeyBART\")"
      ],
      "metadata": {
        "id": "-Z3LuzuU7rsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "SMaREmkCJkLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Find better decoding methods"
      ],
      "metadata": {
        "id": "7neb82xTfBek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode and check device\n",
        "doc_input_ids = tokenizer.encode(df_cnn['article'][0], return_tensors=\"pt\").cuda()\n",
        "doc_input_ids.is_cuda"
      ],
      "metadata": {
        "id": "S2LB_kfwIgmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keyword case1. beam search \n",
        "doc_generated_sequence_1 = model.generate(\n",
        "    input_ids=doc_input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5,\n",
        "    early_stopping=True, #EOS토큰이 나오면 생성을 중단\n",
        "    no_repeat_ngram_size=2 # 2-gram의 어구가 반복되지 않도록 설정함\n",
        "    )"
      ],
      "metadata": {
        "id": "aqqQ_azfVWH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keyword case2. Top-k Sampling & Top-p (nucleus) sampling\n",
        "doc_generated_sequence_2 = model.generate(\n",
        "    input_ids=doc_input_ids,\n",
        "    do_sample=True, #샘플링 전략 사용\n",
        "    max_length=30, # 최대 디코딩 길이는 30\n",
        "    top_k=60, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
        "    top_p=0.95 # 누적 확률이 95%인 후보집합에서만 생성\n",
        "    )"
      ],
      "metadata": {
        "id": "KWBPFdb6ben9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare generated texts\n",
        "doc_output_1 = tokenizer.decode(doc_generated_sequence_1.squeeze(), skip_special_tokens=True)\n",
        "doc_output_2 = tokenizer.decode(doc_generated_sequence_2.squeeze(), skip_special_tokens=True)\n",
        "ls_doc_output_1 = list(filter(None, doc_output_1.split(';')))\n",
        "ls_doc_output_2 = list(filter(None, doc_output_2.split(';')))\n",
        "print(f\"\"\"OUPUT OF THE FIRST CASE : {ls_doc_output_1} \\n\n",
        "OUPUT OF THE SECOND CASE : {ls_doc_output_2}\"\"\")\n",
        "# trust fund;box office;gossip column;sports car;media scrutiny;world wide"
      ],
      "metadata": {
        "id": "33Fx14F3b_S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> 두 번째 방식 채택**"
      ],
      "metadata": {
        "id": "-9HCDQ325D1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try for more documents\n",
        "sample_corpus = df_cnn.article[:5]\n",
        "\n",
        "sample_keywords = []\n",
        "for doc in sample_corpus:\n",
        "    input_ids = tokenizer.encode(doc, padding=True, return_tensors=\"pt\").cuda()\n",
        "    if input_ids.is_cuda == True:\n",
        "        generated_sequence = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            do_sample=True, #샘플링 전략 사용\n",
        "            max_length=30, # 최대 디코딩 길이는 30\n",
        "            top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
        "            top_p=0.95 # 누적 확률이 95%인 후보집합에서만 생성\n",
        "        )\n",
        "        keywords = tokenizer.decode(generated_sequence.squeeze(), skip_special_tokens=True)\n",
        "        ls_keywords = list(filter(None, keywords.split(';')))\n",
        "        sample_keywords.append(ls_keywords)\n",
        "    else :\n",
        "        print('Error occured.')\n",
        "        break"
      ],
      "metadata": {
        "id": "x0AOBLVEok_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " sample_keywords"
      ],
      "metadata": {
        "id": "qnkA86VNpXXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Extract keywords"
      ],
      "metadata": {
        "id": "1xG54q0Ul8fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = list(df_cnn['article'])"
      ],
      "metadata": {
        "id": "pcJdkGAdnCfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_keywords = []\n",
        "\n",
        "sample_keywords = []\n",
        "for doc in sample_corpus:\n",
        "    input_ids = tokenizer.encode(doc, padding=True, return_tensors=\"pt\").cuda()\n",
        "    if input_ids.is_cuda == True:\n",
        "        generated_sequence = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            do_sample=True, #샘플링 전략 사용\n",
        "            max_length=30, # 최대 디코딩 길이는 30\n",
        "            top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
        "            top_p=0.95 # 누적 확률이 95%인 후보집합에서만 생성\n",
        "        )\n",
        "        keywords = tokenizer.decode(generated_sequence.squeeze(), skip_special_tokens=True)\n",
        "        ls_keywords = list(filter(None, keywords.split(';')))\n",
        "        sample_keywords.append(ls_keywords)\n",
        "    else :\n",
        "        print('Error occured.')\n",
        "        break"
      ],
      "metadata": {
        "id": "KDocKQ32NDQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reference (bert) (temp)"
      ],
      "metadata": {
        "id": "o4ugDLa8TZQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"distilroberta-base\"\n",
        "model_2 = AutoModel.from_pretrained(model_name)\n",
        "tokenizer_2 = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "XrQGXya0OCIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_tokens_2 = tokenizer_2(candidates_list[0], padding=True, return_tensors=\"pt\")\n",
        "candidate_embeddings_2 = model_2(**candidate_tokens_2)[\"pooler_output\"]"
      ],
      "metadata": {
        "id": "AysrjVziTcM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_tokens_2.input_ids.shape"
      ],
      "metadata": {
        "id": "wJRBr8ayTvbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_embeddings_2.shape"
      ],
      "metadata": {
        "id": "ruzoxDdRTqU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ouput"
      ],
      "metadata": {
        "id": "hugXhPewYo5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save as json format"
      ],
      "metadata": {
        "id": "K4OHWaOT7r2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWupJLN67r7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQcD0S207sAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFFQVQv77sDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}