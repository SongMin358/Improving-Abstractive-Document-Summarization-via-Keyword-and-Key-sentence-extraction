{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SongMin358/Summurization-via-Multi-task-Learning/blob/main/Step1_Keyword_List_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "R0RE-f04YwUr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdwSDhLY7Tsy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "bgRa7Yg7fg1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "k4bb_AomW_Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "8aihnWpsYe21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
      ],
      "metadata": {
        "id": "eekV7-hi7rfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "EQ8IejQyZT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset type 'datsaet'\n",
        "print(type(dataset))"
      ],
      "metadata": {
        "id": "xv_hsORIXaly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to pandas dataframe\n",
        "df_cnn_train = pd.DataFrame(dataset['train'])\n",
        "df_cnn_valid = pd.DataFrame(dataset['validation'])\n",
        "df_cnn_test = pd.DataFrame(dataset['test'])"
      ],
      "metadata": {
        "id": "W7vSaLGaXeyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate to make full dataframe\n",
        "## 데이터셋 전체에 작업해서 보내드리면 되는 것 같아서, 일단 전부 concat해뒀습니다.\n",
        "df_cnn = pd.concat([df_cnn_train, df_cnn_valid, df_cnn_test], axis=0)\n",
        "df_cnn = df_cnn.reset_index(drop=True)\n",
        "df_cnn.head()"
      ],
      "metadata": {
        "id": "_Nh8tjC1ZdvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn.shape"
      ],
      "metadata": {
        "id": "SSpmSEz_g7Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "HHM2Sy_RYiKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Candidate Selection"
      ],
      "metadata": {
        "id": "oA8kE9ee91BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = list(df_cnn['article'])[:3]\n",
        "# corpus"
      ],
      "metadata": {
        "id": "pVf5zMy78CKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "\n",
        "# CountVecotrizer _ Extract n-gram candidate \n",
        "n_gram_range = (1, 2)\n",
        "stop_words = 'english' # use built-in stop word list for English\n",
        "\n",
        "\n",
        "candidates_list = []\n",
        "\n",
        "for doc in corpus :\n",
        "\n",
        "    # Extract candidate words/phrases\n",
        "    vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words)\n",
        "    count = vectorizer.fit_transform([doc])\n",
        "    all_candidates = vectorizer.get_feature_names_out()\n",
        "    ## ngram_candidates.append(all_candidates)\n",
        "\n",
        "    # Extract only noun words (POS tagging)\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    nlp_doc = nlp(doc)\n",
        "    noun_phrases = set(chunk.text.strip().lower() for chunk in nlp_doc.noun_chunks)\n",
        "    nouns = set()\n",
        "    for token in nlp_doc : \n",
        "        if token.pos_ ==\"NOUN\":\n",
        "            nouns.add(token.text)\n",
        "    all_nouns = nouns.union(noun_phrases) # union noun words/phrases       \n",
        "\n",
        "    # Filter all noun candidate \n",
        "    noun_candidates  =list(filter(lambda candidate: candidate in all_nouns, all_candidates))\n",
        "\n",
        "    candidates_list.append(noun_candidates)"
      ],
      "metadata": {
        "id": "qwWBDcrO5JLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first 10 keywords from the first document\n",
        "candidates_list[0][:10]"
      ],
      "metadata": {
        "id": "Zat-6ATND15I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KeyWord Extraction"
      ],
      "metadata": {
        "id": "a-Q51m2UHE-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "x9g_XJI6hIG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bloomberg/KeyBART\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"bloomberg/KeyBART\")"
      ],
      "metadata": {
        "id": "-Z3LuzuU7rsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "all_keywords = []\n",
        "\n",
        "for i in range(3):\n",
        "    \n",
        "    candidate_tokens = tokenizer(candidates_list[i], padding=True, return_tensors=\"pt\")['input_ids']\n",
        "    candidate_embeddings = model(candidate_tokens)['encoder_last_hidden_state']\n",
        "    article_tokens = tokenizer([df_cnn['article'][i]], padding=True, return_tensors=\"pt\")['input_ids']\n",
        "    article_embedding = model(article_tokens)['encoder_last_hidden_state']\n",
        "\n",
        "    candidate_embeddings = candidate_embeddings.detach().numpy()\n",
        "    article_embedding = article_embedding.detach().numpy()\n",
        "\n",
        "    top_k = 10\n",
        "    distances = cosine_similarity(candidate_embeddings, article_embedding)\n",
        "    keywords = [candidates_list[i][index] for index in distances.argsort()[0][-top_k:]]\n",
        "\n",
        "    all_keywords.append(keywords)"
      ],
      "metadata": {
        "id": "KDocKQ32NDQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNS9S5QvqYNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_tokens = tokenizer(candidates_list[i], padding=True, return_tensors=\"pt\")['input_ids']"
      ],
      "metadata": {
        "id": "quqm4DS_i93o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(candidate_tokens.shape)"
      ],
      "metadata": {
        "id": "C2cPyRiQmVZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_tokens = tokenizer([df_cnn['article'][0]], padding=True, return_tensors=\"pt\")['input_ids']"
      ],
      "metadata": {
        "id": "G625hQzhk8hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(article_tokens.shape)"
      ],
      "metadata": {
        "id": "-MFxKge3mXsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_embeddings = model(candidate_tokens)['encoder_hidden_states']"
      ],
      "metadata": {
        "id": "lwiXBUHzlK08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_embedding = model(article_tokens)['encoder_hidden_states']"
      ],
      "metadata": {
        "id": "hFSO6L0jcEOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reference (bert) (temp)"
      ],
      "metadata": {
        "id": "o4ugDLa8TZQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"distilroberta-base\"\n",
        "model_2 = AutoModel.from_pretrained(model_name)\n",
        "tokenizer_2 = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "XrQGXya0OCIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_tokens_2 = tokenizer_2(candidates_list[0], padding=True, return_tensors=\"pt\")\n",
        "candidate_embeddings_2 = model_2(**candidate_tokens_2)[\"pooler_output\"]"
      ],
      "metadata": {
        "id": "AysrjVziTcM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_tokens_2.input_ids.shape"
      ],
      "metadata": {
        "id": "wJRBr8ayTvbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_embeddings_2.shape"
      ],
      "metadata": {
        "id": "ruzoxDdRTqU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ouput"
      ],
      "metadata": {
        "id": "hugXhPewYo5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save as json format"
      ],
      "metadata": {
        "id": "K4OHWaOT7r2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWupJLN67r7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQcD0S207sAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFFQVQv77sDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}